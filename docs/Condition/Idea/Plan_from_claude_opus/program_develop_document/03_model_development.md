# ë‹¨ê³„ë³„ ëª¨ë¸ ê°œë°œ ê°€ì´ë“œ

## Phase 1: LightGBM í”„ë¡œí† íƒ€ì… (1ì£¼ì°¨)

### 1.1 LightGBM ê¸°ë³¸ ëª¨ë¸ êµ¬í˜„

```python
# models/lightgbm_model.py
import lightgbm as lgb
import numpy as np
import pandas as pd
from sklearn.model_selection import TimeSeriesSplit
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
import optuna
from typing import Dict, Tuple, Optional
import pickle
import logging

class LightGBMTrader:
    """LightGBM ê¸°ë°˜ ê±°ë˜ ëª¨ë¸"""
    
    def __init__(self, task_type: str = 'classification'):
        """
        Args:
            task_type: 'classification' ë˜ëŠ” 'regression'
        """
        self.task_type = task_type
        self.model = None
        self.best_params = None
        self.feature_importance = None
        self.logger = logging.getLogger(__name__)
        
    def optimize_hyperparameters(
        self,
        X_train: pd.DataFrame,
        y_train: pd.Series,
        n_trials: int = 50,
        cv_folds: int = 5
    ) -> Dict:
        """
        Optunaë¥¼ ì‚¬ìš©í•œ ë² ì´ì§€ì•ˆ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”
        
        Args:
            X_train: í•™ìŠµ ë°ì´í„°
            y_train: íƒ€ê²Ÿ ë°ì´í„°
            n_trials: ì‹œë„ íšŸìˆ˜
            cv_folds: êµì°¨ ê²€ì¦ í´ë“œ ìˆ˜
            
        Returns:
            ìµœì  íŒŒë¼ë¯¸í„° ë”•ì…”ë„ˆë¦¬
        """
        def objective(trial):
            params = {
                'objective': 'binary' if self.task_type == 'classification' else 'regression',
                'metric': 'binary_logloss' if self.task_type == 'classification' else 'rmse',
                'boosting_type': 'gbdt',
                'num_leaves': trial.suggest_int('num_leaves', 20, 300),
                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),
                'feature_fraction': trial.suggest_float('feature_fraction', 0.5, 1.0),
                'bagging_fraction': trial.suggest_float('bagging_fraction', 0.5, 1.0),
                'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),
                'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),
                'lambda_l1': trial.suggest_float('lambda_l1', 1e-8, 10.0, log=True),
                'lambda_l2': trial.suggest_float('lambda_l2', 1e-8, 10.0, log=True),
                'max_depth': trial.suggest_int('max_depth', 3, 12),
                'min_gain_to_split': trial.suggest_float('min_gain_to_split', 0, 15),
                'random_state': 42,
                'verbosity': -1
            }
            
            # ì‹œê³„ì—´ êµì°¨ ê²€ì¦
            tscv = TimeSeriesSplit(n_splits=cv_folds)
            scores = []
            
            for train_idx, val_idx in tscv.split(X_train):
                X_fold_train = X_train.iloc[train_idx]
                X_fold_val = X_train.iloc[val_idx]
                y_fold_train = y_train.iloc[train_idx]
                y_fold_val = y_train.iloc[val_idx]
                
                # LightGBM ë°ì´í„°ì…‹ ìƒì„±
                train_data = lgb.Dataset(X_fold_train, label=y_fold_train)
                val_data = lgb.Dataset(X_fold_val, label=y_fold_val, reference=train_data)
                
                # ëª¨ë¸ í•™ìŠµ
                model = lgb.train(
                    params,
                    train_data,
                    valid_sets=[val_data],
                    num_boost_round=1000,
                    callbacks=[
                        lgb.early_stopping(100),
                        lgb.log_evaluation(0)
                    ]
                )
                
                # ì˜ˆì¸¡ ë° í‰ê°€
                predictions = model.predict(X_fold_val, num_iteration=model.best_iteration)
                
                if self.task_type == 'classification':
                    predictions_binary = (predictions > 0.5).astype(int)
                    score = f1_score(y_fold_val, predictions_binary)
                else:
                    score = -np.sqrt(np.mean((predictions - y_fold_val) ** 2))  # RMSE (ìŒìˆ˜)
                
                scores.append(score)
            
            return np.mean(scores)
        
        # Optuna ìŠ¤í„°ë”” ìƒì„± ë° ìµœì í™”
        study = optuna.create_study(
            direction='maximize',
            sampler=optuna.samplers.TPESampler(seed=42)
        )
        
        study.optimize(objective, n_trials=n_trials, show_progress_bar=True)
        
        self.logger.info(f"Best trial: {study.best_trial.params}")
        self.logger.info(f"Best score: {study.best_value}")
        
        self.best_params = study.best_trial.params
        self.best_params['objective'] = 'binary' if self.task_type == 'classification' else 'regression'
        self.best_params['metric'] = 'binary_logloss' if self.task_type == 'classification' else 'rmse'
        self.best_params['random_state'] = 42
        self.best_params['verbosity'] = -1
        
        return self.best_params
    
    def train(
        self,
        X_train: pd.DataFrame,
        y_train: pd.Series,
        X_val: Optional[pd.DataFrame] = None,
        y_val: Optional[pd.Series] = None,
        params: Optional[Dict] = None
    ):
        """
        ëª¨ë¸ í•™ìŠµ
        
        Args:
            X_train: í•™ìŠµ ë°ì´í„°
            y_train: í•™ìŠµ íƒ€ê²Ÿ
            X_val: ê²€ì¦ ë°ì´í„°
            y_val: ê²€ì¦ íƒ€ê²Ÿ
            params: í•˜ì´í¼íŒŒë¼ë¯¸í„° (Noneì´ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©)
        """
        if params is None:
            params = self.best_params if self.best_params else self._get_default_params()
        
        # ë°ì´í„°ì…‹ ìƒì„±
        train_data = lgb.Dataset(X_train, label=y_train)
        valid_sets = [train_data]
        
        if X_val is not None and y_val is not None:
            val_data = lgb.Dataset(X_val, label=y_val, reference=train_data)
            valid_sets.append(val_data)
        
        # ëª¨ë¸ í•™ìŠµ
        self.model = lgb.train(
            params,
            train_data,
            valid_sets=valid_sets,
            num_boost_round=2000,
            callbacks=[
                lgb.early_stopping(100),
                lgb.log_evaluation(100)
            ]
        )
        
        # íŠ¹ì„± ì¤‘ìš”ë„ ì €ì¥
        self.feature_importance = pd.DataFrame({
            'feature': X_train.columns,
            'importance': self.model.feature_importance(importance_type='gain')
        }).sort_values('importance', ascending=False)
        
        self.logger.info(f"Training completed. Best iteration: {self.model.best_iteration}")
        self.logger.info(f"Top 10 features:\n{self.feature_importance.head(10)}")
    
    def predict(self, X: pd.DataFrame) -> np.ndarray:
        """ì˜ˆì¸¡"""
        if self.model is None:
            raise ValueError("Model not trained yet")
        
        predictions = self.model.predict(X, num_iteration=self.model.best_iteration)
        
        if self.task_type == 'classification':
            return predictions  # í™•ë¥ ê°’ ë°˜í™˜
        else:
            return predictions
    
    def predict_proba(self, X: pd.DataFrame) -> np.ndarray:
        """í™•ë¥  ì˜ˆì¸¡ (ë¶„ë¥˜ ì „ìš©)"""
        if self.task_type != 'classification':
            raise ValueError("predict_proba is only for classification")
        
        return self.predict(X)
    
    def evaluate(self, X: pd.DataFrame, y: pd.Series) -> Dict:
        """ëª¨ë¸ í‰ê°€"""
        predictions = self.predict(X)
        
        if self.task_type == 'classification':
            predictions_binary = (predictions > 0.5).astype(int)
            
            metrics = {
                'accuracy': accuracy_score(y, predictions_binary),
                'precision': precision_score(y, predictions_binary),
                'recall': recall_score(y, predictions_binary),
                'f1': f1_score(y, predictions_binary),
                'auc': roc_auc_score(y, predictions)
            }
        else:
            metrics = {
                'rmse': np.sqrt(np.mean((predictions - y) ** 2)),
                'mae': np.mean(np.abs(predictions - y)),
                'mape': np.mean(np.abs((y - predictions) / y)) * 100
            }
        
        return metrics
    
    def save(self, filepath: str):
        """ëª¨ë¸ ì €ì¥"""
        model_data = {
            'model': self.model,
            'best_params': self.best_params,
            'feature_importance': self.feature_importance,
            'task_type': self.task_type
        }
        
        with open(filepath, 'wb') as f:
            pickle.dump(model_data, f)
        
        self.logger.info(f"Model saved to {filepath}")
    
    @classmethod
    def load(cls, filepath: str) -> 'LightGBMTrader':
        """ëª¨ë¸ ë¡œë“œ"""
        with open(filepath, 'rb') as f:
            model_data = pickle.load(f)
        
        instance = cls(task_type=model_data['task_type'])
        instance.model = model_data['model']
        instance.best_params = model_data['best_params']
        instance.feature_importance = model_data['feature_importance']
        
        return instance
    
    def _get_default_params(self) -> Dict:
        """ê¸°ë³¸ íŒŒë¼ë¯¸í„°"""
        return {
            'objective': 'binary' if self.task_type == 'classification' else 'regression',
            'metric': 'binary_logloss' if self.task_type == 'classification' else 'rmse',
            'boosting_type': 'gbdt',
            'num_leaves': 31,
            'learning_rate': 0.05,
            'feature_fraction': 0.9,
            'bagging_fraction': 0.8,
            'bagging_freq': 5,
            'random_state': 42,
            'verbosity': -1
        }
```

### 1.2 Phase 1 ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸

```python
# scripts/train_phase1.py
import sys
import logging
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
sys.path.append(str(Path(__file__).parent.parent))

from data.pipeline import DataPipeline
from models.lightgbm_model import LightGBMTrader
import pandas as pd
import numpy as np

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

def main():
    # 1. ë°ì´í„° ì¤€ë¹„
    print("=" * 50)
    print("Phase 1: LightGBM í”„ë¡œí† íƒ€ì…")
    print("=" * 50)
    
    print("\n1. ë°ì´í„° ë¡œë”©...")
    pipeline = DataPipeline(
        db_path='./data/stock_data.db',
        cache_dir='./cache'
    )
    
    stock_codes = ['005930', '000660']  # ì‚¼ì„±ì „ì, SKí•˜ì´ë‹‰ìŠ¤
    X_train, X_test, y_train, y_test = pipeline.prepare_training_data(
        stock_codes=stock_codes,
        start_date='20220101000000',
        end_date='20231231235959',
        target_holding_period=10,
        test_split=0.2
    )
    
    print(f"í•™ìŠµ ë°ì´í„°: {X_train.shape}")
    print(f"í…ŒìŠ¤íŠ¸ ë°ì´í„°: {X_test.shape}")
    print(f"íƒ€ê²Ÿ ë¶„í¬:\n{y_train.value_counts(normalize=True)}")
    
    # 2. í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”
    print("\n2. í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”...")
    model = LightGBMTrader(task_type='classification')
    
    best_params = model.optimize_hyperparameters(
        X_train, y_train,
        n_trials=20,  # ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ ì ì€ ìˆ˜ë¡œ ì„¤ì •
        cv_folds=3
    )
    
    print(f"ìµœì  íŒŒë¼ë¯¸í„°: {best_params}")
    
    # 3. ëª¨ë¸ í•™ìŠµ
    print("\n3. ëª¨ë¸ í•™ìŠµ...")
    model.train(X_train, y_train, X_test, y_test, params=best_params)
    
    # 4. í‰ê°€
    print("\n4. ëª¨ë¸ í‰ê°€...")
    train_metrics = model.evaluate(X_train, y_train)
    test_metrics = model.evaluate(X_test, y_test)
    
    print("í•™ìŠµ ë°ì´í„° ì„±ëŠ¥:")
    for metric, value in train_metrics.items():
        print(f"  {metric}: {value:.4f}")
    
    print("\ní…ŒìŠ¤íŠ¸ ë°ì´í„° ì„±ëŠ¥:")
    for metric, value in test_metrics.items():
        print(f"  {metric}: {value:.4f}")
    
    # 5. íŠ¹ì„± ì¤‘ìš”ë„
    print("\n5. Top 20 ì¤‘ìš” íŠ¹ì„±:")
    print(model.feature_importance.head(20))
    
    # 6. ëª¨ë¸ ì €ì¥
    print("\n6. ëª¨ë¸ ì €ì¥...")
    model.save('./models/lightgbm_phase1.pkl')
    
    print("\nâœ… Phase 1 ì™„ë£Œ!")
    
if __name__ == "__main__":
    main()
```

## Phase 2: LSTM ë”¥ëŸ¬ë‹ ëª¨ë¸ (2-3ì£¼ì°¨)

### 2.1 LSTM ëª¨ë¸ êµ¬í˜„

```python
# models/lstm_model.py
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader, TensorDataset
import numpy as np
import pandas as pd
from typing import Tuple, Optional, Dict
from sklearn.preprocessing import StandardScaler
import logging

class StockDataset(Dataset):
    """ì£¼ì‹ ë°ì´í„°ì…‹"""
    
    def __init__(self, X: np.ndarray, y: np.ndarray, seq_length: int = 60):
        """
        Args:
            X: íŠ¹ì„± ë°ì´í„° (n_samples, n_features)
            y: íƒ€ê²Ÿ ë°ì´í„° (n_samples,)
            seq_length: ì‹œí€€ìŠ¤ ê¸¸ì´
        """
        self.seq_length = seq_length
        self.X = X
        self.y = y
        
    def __len__(self):
        return len(self.X) - self.seq_length + 1
    
    def __getitem__(self, idx):
        # ì‹œí€€ìŠ¤ ë°ì´í„° ìƒì„±
        X_seq = self.X[idx:idx + self.seq_length]
        y_target = self.y[idx + self.seq_length - 1]
        
        return torch.FloatTensor(X_seq), torch.FloatTensor([y_target])

class LSTMModel(nn.Module):
    """LSTM ê±°ë˜ ëª¨ë¸"""
    
    def __init__(
        self,
        input_dim: int,
        hidden_dim: int = 256,
        num_layers: int = 3,
        dropout: float = 0.2,
        bidirectional: bool = True
    ):
        super(LSTMModel, self).__init__()
        
        self.hidden_dim = hidden_dim
        self.num_layers = num_layers
        self.bidirectional = bidirectional
        
        # LSTM ë ˆì´ì–´
        self.lstm = nn.LSTM(
            input_dim,
            hidden_dim,
            num_layers,
            batch_first=True,
            dropout=dropout if num_layers > 1 else 0,
            bidirectional=bidirectional
        )
        
        # Attention ë©”ì»¤ë‹ˆì¦˜
        lstm_output_dim = hidden_dim * 2 if bidirectional else hidden_dim
        self.attention = nn.MultiheadAttention(
            lstm_output_dim,
            num_heads=8,
            dropout=dropout,
            batch_first=True
        )
        
        # Fully Connected ë ˆì´ì–´
        self.fc_layers = nn.Sequential(
            nn.Linear(lstm_output_dim, 256),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.BatchNorm1d(256),
            
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.BatchNorm1d(128),
            
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Dropout(dropout),
            
            nn.Linear(64, 1),
            nn.Sigmoid()
        )
        
    def forward(self, x):
        # LSTM
        lstm_out, (hidden, cell) = self.lstm(x)
        
        # Self-Attention
        attn_out, _ = self.attention(lstm_out, lstm_out, lstm_out)
        
        # ë§ˆì§€ë§‰ íƒ€ì„ìŠ¤í… ë˜ëŠ” í‰ê·  í’€ë§
        # out = attn_out[:, -1, :]  # ë§ˆì§€ë§‰ íƒ€ì„ìŠ¤í…
        out = torch.mean(attn_out, dim=1)  # í‰ê·  í’€ë§
        
        # Fully Connected
        out = self.fc_layers(out)
        
        return out

class LSTMTrader:
    """LSTM íŠ¸ë ˆì´ë”"""
    
    def __init__(
        self,
        input_dim: int,
        seq_length: int = 60,
        device: str = 'cuda' if torch.cuda.is_available() else 'cpu'
    ):
        self.input_dim = input_dim
        self.seq_length = seq_length
        self.device = torch.device(device)
        self.model = None
        self.scaler = StandardScaler()
        self.logger = logging.getLogger(__name__)
        
        self.logger.info(f"Using device: {self.device}")
    
    def build_model(
        self,
        hidden_dim: int = 256,
        num_layers: int = 3,
        dropout: float = 0.2
    ):
        """ëª¨ë¸ êµ¬ì¶•"""
        self.model = LSTMModel(
            self.input_dim,
            hidden_dim,
            num_layers,
            dropout
        ).to(self.device)
        
        self.logger.info(f"Model built with {sum(p.numel() for p in self.model.parameters())} parameters")
    
    def train(
        self,
        X_train: np.ndarray,
        y_train: np.ndarray,
        X_val: Optional[np.ndarray] = None,
        y_val: Optional[np.ndarray] = None,
        epochs: int = 100,
        batch_size: int = 64,
        learning_rate: float = 0.001,
        early_stopping_patience: int = 10
    ):
        """ëª¨ë¸ í•™ìŠµ"""
        # ë°ì´í„° ì •ê·œí™”
        X_train = self.scaler.fit_transform(X_train)
        if X_val is not None:
            X_val = self.scaler.transform(X_val)
        
        # ë°ì´í„°ì…‹ ìƒì„±
        train_dataset = StockDataset(X_train, y_train, self.seq_length)
        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
        
        if X_val is not None and y_val is not None:
            val_dataset = StockDataset(X_val, y_val, self.seq_length)
            val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
        
        # ëª¨ë¸ì´ ì—†ìœ¼ë©´ ìƒì„±
        if self.model is None:
            self.build_model()
        
        # ì˜µí‹°ë§ˆì´ì € ë° ì†ì‹¤ í•¨ìˆ˜
        optimizer = optim.AdamW(self.model.parameters(), lr=learning_rate, weight_decay=0.01)
        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)
        criterion = nn.BCELoss()
        
        # í•™ìŠµ
        best_val_loss = float('inf')
        patience_counter = 0
        
        for epoch in range(epochs):
            # Training
            self.model.train()
            train_loss = 0
            train_correct = 0
            train_total = 0
            
            for batch_x, batch_y in train_loader:
                batch_x = batch_x.to(self.device)
                batch_y = batch_y.to(self.device)
                
                optimizer.zero_grad()
                outputs = self.model(batch_x)
                loss = criterion(outputs, batch_y)
                loss.backward()
                
                # Gradient clipping
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
                
                optimizer.step()
                
                train_loss += loss.item()
                predicted = (outputs > 0.5).float()
                train_correct += (predicted == batch_y).sum().item()
                train_total += batch_y.size(0)
            
            avg_train_loss = train_loss / len(train_loader)
            train_accuracy = train_correct / train_total
            
            # Validation
            if X_val is not None:
                self.model.eval()
                val_loss = 0
                val_correct = 0
                val_total = 0
                
                with torch.no_grad():
                    for batch_x, batch_y in val_loader:
                        batch_x = batch_x.to(self.device)
                        batch_y = batch_y.to(self.device)
                        
                        outputs = self.model(batch_x)
                        loss = criterion(outputs, batch_y)
                        
                        val_loss += loss.item()
                        predicted = (outputs > 0.5).float()
                        val_correct += (predicted == batch_y).sum().item()
                        val_total += batch_y.size(0)
                
                avg_val_loss = val_loss / len(val_loader)
                val_accuracy = val_correct / val_total
                
                # Early stopping
                if avg_val_loss < best_val_loss:
                    best_val_loss = avg_val_loss
                    patience_counter = 0
                    # ìµœê³  ëª¨ë¸ ì €ì¥
                    torch.save(self.model.state_dict(), 'best_lstm_model.pth')
                else:
                    patience_counter += 1
                
                if patience_counter >= early_stopping_patience:
                    self.logger.info(f"Early stopping at epoch {epoch}")
                    break
                
                self.logger.info(
                    f"Epoch {epoch+1}/{epochs} - "
                    f"Train Loss: {avg_train_loss:.4f}, Train Acc: {train_accuracy:.4f}, "
                    f"Val Loss: {avg_val_loss:.4f}, Val Acc: {val_accuracy:.4f}"
                )
            else:
                self.logger.info(
                    f"Epoch {epoch+1}/{epochs} - "
                    f"Train Loss: {avg_train_loss:.4f}, Train Acc: {train_accuracy:.4f}"
                )
            
            scheduler.step()
        
        # ìµœê³  ëª¨ë¸ ë¡œë“œ
        if X_val is not None:
            self.model.load_state_dict(torch.load('best_lstm_model.pth'))
    
    def predict(self, X: np.ndarray) -> np.ndarray:
        """ì˜ˆì¸¡"""
        self.model.eval()
        
        # ì •ê·œí™”
        X = self.scaler.transform(X)
        
        # ë°ì´í„°ì…‹ ìƒì„±
        dataset = StockDataset(X, np.zeros(len(X)), self.seq_length)
        loader = DataLoader(dataset, batch_size=64, shuffle=False)
        
        predictions = []
        
        with torch.no_grad():
            for batch_x, _ in loader:
                batch_x = batch_x.to(self.device)
                outputs = self.model(batch_x)
                predictions.extend(outputs.cpu().numpy())
        
        return np.array(predictions).flatten()
    
    def save(self, filepath: str):
        """ëª¨ë¸ ì €ì¥"""
        torch.save({
            'model_state_dict': self.model.state_dict(),
            'scaler': self.scaler,
            'input_dim': self.input_dim,
            'seq_length': self.seq_length
        }, filepath)
        
        self.logger.info(f"Model saved to {filepath}")
    
    def load(self, filepath: str):
        """ëª¨ë¸ ë¡œë“œ"""
        checkpoint = torch.load(filepath, map_location=self.device)
        
        self.input_dim = checkpoint['input_dim']
        self.seq_length = checkpoint['seq_length']
        self.scaler = checkpoint['scaler']
        
        self.build_model()
        self.model.load_state_dict(checkpoint['model_state_dict'])
        
        self.logger.info(f"Model loaded from {filepath}")
```

### 2.2 GPU ìµœì í™” ì ìš©

```python
# models/gpu_optimizer.py
import torch
from torch.cuda.amp import autocast, GradScaler
import cupy as cp
import numpy as np
from typing import Tuple

class GPUOptimizer:
    """GPU ìµœì í™” ìœ í‹¸ë¦¬í‹°"""
    
    @staticmethod
    def check_gpu_availability():
        """GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸"""
        print("=" * 50)
        print("GPU ì •ë³´")
        print("=" * 50)
        
        if torch.cuda.is_available():
            print(f"PyTorch CUDA ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.is_available()}")
            print(f"CUDA ë²„ì „: {torch.version.cuda}")
            print(f"GPU ê°œìˆ˜: {torch.cuda.device_count()}")
            
            for i in range(torch.cuda.device_count()):
                print(f"\nGPU {i}: {torch.cuda.get_device_name(i)}")
                print(f"  ë©”ëª¨ë¦¬: {torch.cuda.get_device_properties(i).total_memory / 1024**3:.2f} GB")
                print(f"  Compute Capability: {torch.cuda.get_device_properties(i).major}.{torch.cuda.get_device_properties(i).minor}")
        else:
            print("CUDAë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. CPU ëª¨ë“œë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.")
        
        try:
            import cupy as cp
            print(f"\nCuPy ì‚¬ìš© ê°€ëŠ¥: True")
            print(f"CuPy ë²„ì „: {cp.__version__}")
        except ImportError:
            print("\nCuPyë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    @staticmethod
    def optimize_batch_size(model, sample_input_shape: Tuple, max_batch_size: int = 512):
        """ìµœì  ë°°ì¹˜ í¬ê¸° ì°¾ê¸°"""
        device = next(model.parameters()).device
        batch_size = 1
        optimal_batch_size = 1
        
        while batch_size <= max_batch_size:
            try:
                # í…ŒìŠ¤íŠ¸ ì…ë ¥ ìƒì„±
                test_input = torch.randn(batch_size, *sample_input_shape).to(device)
                
                # Forward pass
                with torch.no_grad():
                    _ = model(test_input)
                
                optimal_batch_size = batch_size
                batch_size *= 2
                
                # ë©”ëª¨ë¦¬ ì •ë¦¬
                del test_input
                torch.cuda.empty_cache()
                
            except RuntimeError as e:
                if "out of memory" in str(e):
                    print(f"ë©”ëª¨ë¦¬ ë¶€ì¡±: ë°°ì¹˜ í¬ê¸° {batch_size}")
                    break
                else:
                    raise e
        
        print(f"ìµœì  ë°°ì¹˜ í¬ê¸°: {optimal_batch_size}")
        return optimal_batch_size
    
    @staticmethod
    def mixed_precision_training(model, train_loader, optimizer, criterion, device):
        """Mixed Precision Training ì˜ˆì œ"""
        scaler = GradScaler()
        
        model.train()
        for batch_x, batch_y in train_loader:
            batch_x = batch_x.to(device)
            batch_y = batch_y.to(device)
            
            optimizer.zero_grad()
            
            # Mixed precision
            with autocast():
                outputs = model(batch_x)
                loss = criterion(outputs, batch_y)
            
            # Backward pass with scaling
            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()
        
        return loss.item()
```

## Phase 3: ì•™ìƒë¸” ë° ê³ ê¸‰ ê¸°ëŠ¥ (4-5ì£¼ì°¨)

### 3.1 ì•™ìƒë¸” ëª¨ë¸

```python
# models/ensemble.py
import numpy as np
import pandas as pd
from typing import List, Dict, Optional
import pickle
from sklearn.ensemble import VotingClassifier
from sklearn.base import BaseEstimator, ClassifierMixin

class EnsembleTrader(BaseEstimator, ClassifierMixin):
    """ì•™ìƒë¸” ê±°ë˜ ëª¨ë¸"""
    
    def __init__(
        self,
        models: Dict[str, any],
        weights: Optional[Dict[str, float]] = None,
        voting: str = 'soft'
    ):
        """
        Args:
            models: ëª¨ë¸ ë”•ì…”ë„ˆë¦¬ {'model_name': model_instance}
            weights: ëª¨ë¸ë³„ ê°€ì¤‘ì¹˜
            voting: 'hard' ë˜ëŠ” 'soft'
        """
        self.models = models
        self.weights = weights or {name: 1.0 for name in models.keys()}
        self.voting = voting
        
    def fit(self, X, y):
        """í•™ìŠµ (ì´ë¯¸ í•™ìŠµëœ ëª¨ë¸ ì‚¬ìš©)"""
        return self
    
    def predict_proba(self, X):
        """í™•ë¥  ì˜ˆì¸¡"""
        predictions = {}
        
        for name, model in self.models.items():
            if hasattr(model, 'predict_proba'):
                pred = model.predict_proba(X)
            else:
                pred = model.predict(X)
            
            # ì´ì§„ ë¶„ë¥˜ì¸ ê²½ìš° positive class í™•ë¥ ë§Œ
            if len(pred.shape) == 1:
                predictions[name] = pred
            else:
                predictions[name] = pred[:, 1]
        
        # ê°€ì¤‘ í‰ê· 
        weighted_preds = np.zeros(len(X))
        total_weight = sum(self.weights.values())
        
        for name, pred in predictions.items():
            weighted_preds += pred * self.weights[name] / total_weight
        
        # ì´ì§„ ë¶„ë¥˜ìš© í˜•íƒœë¡œ ë³€í™˜
        proba = np.column_stack([1 - weighted_preds, weighted_preds])
        
        return proba
    
    def predict(self, X):
        """ì˜ˆì¸¡"""
        if self.voting == 'soft':
            proba = self.predict_proba(X)
            return (proba[:, 1] > 0.5).astype(int)
        else:
            # Hard voting
            predictions = []
            for name, model in self.models.items():
                pred = model.predict(X)
                predictions.append(pred)
            
            # ë‹¤ìˆ˜ê²°
            predictions = np.array(predictions)
            return np.apply_along_axis(
                lambda x: np.bincount(x).argmax(), 
                axis=0, 
                arr=predictions
            )
    
    def evaluate(self, X, y):
        """í‰ê°€"""
        from sklearn.metrics import classification_report, confusion_matrix
        
        y_pred = self.predict(X)
        y_proba = self.predict_proba(X)[:, 1]
        
        print("Confusion Matrix:")
        print(confusion_matrix(y, y_pred))
        print("\nClassification Report:")
        print(classification_report(y, y_pred))
        
        # ê°œë³„ ëª¨ë¸ ì„±ëŠ¥
        print("\nê°œë³„ ëª¨ë¸ ì„±ëŠ¥:")
        for name, model in self.models.items():
            if hasattr(model, 'evaluate'):
                metrics = model.evaluate(X, y)
                print(f"\n{name}:")
                for metric, value in metrics.items():
                    print(f"  {metric}: {value:.4f}")
```

### 3.2 í†µí•© ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸

```python
# scripts/train_full_pipeline.py
import sys
import logging
from pathlib import Path
import numpy as np
import pandas as pd

sys.path.append(str(Path(__file__).parent.parent))

from data.pipeline import DataPipeline
from models.lightgbm_model import LightGBMTrader
from models.lstm_model import LSTMTrader
from models.ensemble import EnsembleTrader
from models.gpu_optimizer import GPUOptimizer

def main():
    print("=" * 60)
    print("STOM ML/DL ë°±í…ŒìŠ¤íŒ… ìµœì í™” ì‹œìŠ¤í…œ - ì „ì²´ íŒŒì´í”„ë¼ì¸")
    print("=" * 60)
    
    # GPU í™•ì¸
    GPUOptimizer.check_gpu_availability()
    
    # 1. ë°ì´í„° ì¤€ë¹„
    print("\n" + "=" * 60)
    print("1. ë°ì´í„° ì¤€ë¹„")
    print("=" * 60)
    
    pipeline = DataPipeline(
        db_path='./data/stock_data.db',
        cache_dir='./cache'
    )
    
    stock_codes = ['005930', '000660', '035720']
    X_train, X_test, y_train, y_test = pipeline.prepare_training_data(
        stock_codes=stock_codes,
        start_date='20220101000000',
        end_date='20231231235959',
        target_holding_period=10,
        test_split=0.2
    )
    
    print(f"âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ")
    print(f"  - í•™ìŠµ: {X_train.shape}")
    print(f"  - í…ŒìŠ¤íŠ¸: {X_test.shape}")
    
    # 2. LightGBM ëª¨ë¸
    print("\n" + "=" * 60)
    print("2. LightGBM ëª¨ë¸ í•™ìŠµ")
    print("=" * 60)
    
    lgb_model = LightGBMTrader(task_type='classification')
    
    # í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” (ê°„ë‹¨í•˜ê²Œ)
    lgb_model.train(X_train, y_train, X_test, y_test)
    lgb_metrics = lgb_model.evaluate(X_test, y_test)
    
    print("âœ… LightGBM ì„±ëŠ¥:")
    for metric, value in lgb_metrics.items():
        print(f"  - {metric}: {value:.4f}")
    
    # 3. LSTM ëª¨ë¸
    print("\n" + "=" * 60)
    print("3. LSTM ëª¨ë¸ í•™ìŠµ")
    print("=" * 60)
    
    lstm_model = LSTMTrader(
        input_dim=X_train.shape[1],
        seq_length=30
    )
    
    # NumPy ë°°ì—´ë¡œ ë³€í™˜
    X_train_np = X_train.values if hasattr(X_train, 'values') else X_train
    X_test_np = X_test.values if hasattr(X_test, 'values') else X_test
    y_train_np = y_train.values if hasattr(y_train, 'values') else y_train
    y_test_np = y_test.values if hasattr(y_test, 'values') else y_test
    
    # í•™ìŠµ ë°ì´í„°ê°€ ì¶©ë¶„í•œì§€ í™•ì¸
    if len(X_train_np) > lstm_model.seq_length:
        lstm_model.train(
            X_train_np, y_train_np,
            X_test_np, y_test_np,
            epochs=20,  # ë¹ ë¥¸ í…ŒìŠ¤íŠ¸
            batch_size=32
        )
        
        # LSTM ì˜ˆì¸¡
        lstm_pred = lstm_model.predict(X_test_np)
        lstm_acc = np.mean((lstm_pred > 0.5) == y_test_np)
        
        print(f"âœ… LSTM ì •í™•ë„: {lstm_acc:.4f}")
    else:
        print("âš ï¸ LSTM í•™ìŠµì„ ìœ„í•œ ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤.")
        lstm_model = None
    
    # 4. ì•™ìƒë¸” ëª¨ë¸
    print("\n" + "=" * 60)
    print("4. ì•™ìƒë¸” ëª¨ë¸")
    print("=" * 60)
    
    models = {'LightGBM': lgb_model}
    weights = {'LightGBM': 1.0}
    
    if lstm_model is not None:
        models['LSTM'] = lstm_model
        weights['LSTM'] = 0.8
    
    ensemble = EnsembleTrader(
        models=models,
        weights=weights,
        voting='soft'
    )
    
    # ì•™ìƒë¸” í‰ê°€
    ensemble.evaluate(X_test, y_test)
    
    # 5. ìµœì¢… ê²°ê³¼
    print("\n" + "=" * 60)
    print("5. ìµœì¢… ê²°ê³¼ ìš”ì•½")
    print("=" * 60)
    
    print("\nâœ… ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì™„ë£Œ!")
    print("\nì£¼ìš” ì„±ê³¼:")
    print(f"  - LightGBM F1 Score: {lgb_metrics['f1']:.4f}")
    if lstm_model:
        print(f"  - LSTM Accuracy: {lstm_acc:.4f}")
    print(f"  - íŠ¹ì„± ê°œìˆ˜: {X_train.shape[1]}")
    print(f"  - í•™ìŠµ ë°ì´í„°: {X_train.shape[0]} ìƒ˜í”Œ")
    
    print("\nğŸ“Š ë‹¤ìŒ ë‹¨ê³„:")
    print("  1. ë°±í…ŒìŠ¤íŒ… ì‹œë®¬ë ˆì´ì…˜ ì‹¤í–‰")
    print("  2. ì‹¤ì‹œê°„ ì˜ˆì¸¡ API êµ¬ì¶•")
    print("  3. ì„±ê³¼ ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ ê°œë°œ")

if __name__ == "__main__":
    main()
```

## ì‹¤í–‰ ê°€ì´ë“œ

### í™˜ê²½ ì„¤ì •

```bash
# 1. ê°€ìƒí™˜ê²½ ìƒì„±
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

# 2. í•„ìˆ˜ íŒ¨í‚¤ì§€ ì„¤ì¹˜
pip install -r requirements.txt

# 3. CUDA ì„¤ì • (GPU ì‚¬ìš© ì‹œ)
# NVIDIA ë“œë¼ì´ë²„ ë° CUDA Toolkit 11.8+ ì„¤ì¹˜ í•„ìš”
```

### requirements.txt

```txt
# Core
pandas>=1.5.0
numpy>=1.24.0
scikit-learn>=1.3.0
sqlite3

# Machine Learning
lightgbm>=4.0.0
xgboost>=1.7.0
optuna>=3.3.0

# Deep Learning
torch>=2.0.0
torchvision>=0.15.0

# GPU Acceleration
cupy-cuda118>=12.0.0  # CUDA ë²„ì „ì— ë§ê²Œ ì¡°ì •
cudf-cu118>=23.0.0    # Optional

# Technical Indicators
ta-lib>=0.4.0

# Visualization
matplotlib>=3.6.0
seaborn>=0.12.0
plotly>=5.14.0

# API
fastapi>=0.100.0
uvicorn>=0.23.0

# Utils
joblib>=1.3.0
tqdm>=4.65.0
python-dotenv>=1.0.0
```

### ë‹¨ê³„ë³„ ì‹¤í–‰

```bash
# Phase 1: LightGBM
python scripts/train_phase1.py

# Phase 2: LSTM (GPU ê¶Œì¥)
python scripts/train_lstm.py

# Phase 3: ì „ì²´ íŒŒì´í”„ë¼ì¸
python scripts/train_full_pipeline.py
```

## ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬

### ì˜ˆìƒ ì„±ëŠ¥ (RTX 3080 ê¸°ì¤€)

| ì‘ì—… | CPU | GPU | ì†ë„ í–¥ìƒ |
|------|-----|-----|----------|
| ë°ì´í„° ë¡œë“œ (100ë§Œ í–‰) | 10ì´ˆ | 10ì´ˆ | 1x |
| íŠ¹ì„± ìƒì„± (100ë§Œ í–‰) | 60ì´ˆ | 15ì´ˆ | 4x |
| LightGBM í•™ìŠµ | 300ì´ˆ | 120ì´ˆ | 2.5x |
| LSTM í•™ìŠµ (100 epochs) | 3600ì´ˆ | 300ì´ˆ | 12x |
| ì˜ˆì¸¡ (10ë§Œ í–‰) | 5ì´ˆ | 1ì´ˆ | 5x |

## ë‹¤ìŒ ë‹¨ê³„

1. **ë°±í…ŒìŠ¤íŒ… ì—”ì§„ í†µí•©**
   - ì‹¤ì œ ê±°ë˜ ì‹œë®¬ë ˆì´ì…˜
   - ìˆ˜ìˆ˜ë£Œ ë° ìŠ¬ë¦¬í”¼ì§€ ê³ ë ¤
   - ë¦¬ìŠ¤í¬ ê´€ë¦¬ ì ìš©

2. **ì‹¤ì‹œê°„ ì˜ˆì¸¡ ì‹œìŠ¤í…œ**
   - FastAPI ê¸°ë°˜ ì˜ˆì¸¡ ì„œë²„
   - ì›¹ì†Œì¼“ ì‹¤ì‹œê°„ ë°ì´í„° ì²˜ë¦¬
   - ë ˆì´í„´ì‹œ ìµœì í™”

3. **ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ**
   - Streamlit ê¸°ë°˜ UI
   - ì‹¤ì‹œê°„ ì„±ê³¼ ì¶”ì 
   - ëª¨ë¸ ë””ë²„ê¹… ë„êµ¬
