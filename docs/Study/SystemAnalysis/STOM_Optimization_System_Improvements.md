# ğŸ¯ STOM ìµœì í™” ì‹œìŠ¤í…œ ê°œì„  ë°©ì•ˆ

## ğŸ“Š í˜„ì¬ ì‹œìŠ¤í…œ êµ¬ì¡° ë¶„ì„

### ìµœì í™” ë°©ë²• 3ê°€ì§€
1. **ê·¸ë¦¬ë“œ ìµœì í™” (OptimizeGrid)**
   - ëª¨ë“  ë³€ìˆ˜ ì¡°í•© ìˆœì°¨ íƒìƒ‰
   - ë²”ìœ„ ìë™ ê´€ë¦¬: `hstd/4` ì´í•˜ ì‚­ì œ
   - ìµœì ê°’ì´ ê²½ê³„ë©´ í™•ì¥

2. **Optuna ë² ì´ì§€ì•ˆ ìµœì í™” (OptimizeOptuna)**
   - `TPESampler`(ê¸°ë³¸), `CmaEsSampler`, `QMCSampler`
   - Early Stopping: `best + len_vars` íšŸìˆ˜
   - ì¤‘ë³µ íƒìƒ‰ ë°©ì§€(`dict_simple_vars`)

3. **GA ìœ ì „ ì•Œê³ ë¦¬ì¦˜ (OptimizeGeneticAlgorithm)**
   - ë³€ìˆ˜ ê°œìˆ˜ Ã— 10íšŒ ë¬´ì‘ìœ„ ìƒ˜í”Œë§
   - ìƒìœ„ 5% ë²”ìœ„ ìˆ˜ë ´

### êµì°¨ê²€ì¦ MERGE ê³„ì‚°
- `backtester/back_static.py:454-473`
- ê³µì‹: `std = Î£(TRAIN[i] Ã— VALID[i] Ã— weight[i]) / count`

**weight (exponential=True)**  
- 6ë¶„í• : `2.00, 1.66, 1.33, 1.00, 0.66, 0.33` (ìµœê·¼ â†’ ê³¼ê±°)  
- 3ë¶„í• : `2.00, 1.33, 0.66`  
- 1ë¶„í• : `1.00` (ê°€ì¤‘ì¹˜ ì—†ìŒ)

---

## ğŸš€ 15ê°€ì§€ ê°œì„  ë°©ì•ˆ

### â­ 1. êµì°¨ê²€ì¦ MERGE ê³„ì‚° ë°©ì‹ ê°œì„  (ìµœìš°ì„ )
**í˜„ì¬ ë¬¸ì œ**  
- TRAIN Ã— VALID ê³±ì…ˆ â†’ ê·¹ë‹¨ê°’ ì¦í­  
  - TRAIN1: 100, VALID1: 200 â†’ 20,000  
  - TRAIN2: 150, VALID2: 140 â†’ 21,000 (ë” ê· í˜•ì¡íŒë° ë‚®ì€ ì ìˆ˜)

**ê°œì„ ì•ˆ 1: ì¡°í™” í‰ê·  (Harmonic Mean)**
```python
def GetOptiValidStd(train_data, valid_data, optistd, betting, exponential):
    std = 0
    count = len(train_data)
    for i in range(count):
        ex = (count - i) * 2 / count if exponential and count > 1 else 1.0
        
        # ì¡°í™” í‰ê· : 2 / (1/TRAIN + 1/VALID)
        if train_data[i] != 0 and valid_data[i] != 0:
            harmonic = 2 * train_data[i] * valid_data[i] / (train_data[i] + valid_data[i])
        else:
            harmonic = 0
        
        std += harmonic * ex
    
    std = round(std / count / betting, 2) if optistd == 'TG' else round(std / count, 2)
    return std
```

**ê°œì„ ì•ˆ 2: ê°€ì¤‘ í‰ê·  (Weighted Average)**  
- TRAIN 70% + VALID 30% (ì¼ë°˜í™” ì¤‘ì‹œ)  
`std_ = (train_data[i] * 0.7 + valid_data[i] * 0.3) * ex`

**ê°œì„ ì•ˆ 3: ìµœì†Œê°’ ê¸°ë°˜ (Conservative)**  
- ë‘˜ ì¤‘ ë‚®ì€ ê°’ ì„ íƒ (ì•ˆì •ì„± ì¤‘ì‹œ)  
`std_ = min(train_data[i], valid_data[i]) * ex`

**íš¨ê³¼**: ê·¹ë‹¨ê°’ ì¦í­ ë°©ì§€, ê· í˜•ì¡íŒ ì „ëµ ì„ íƒ

---

### â­ 2. ë²”ìœ„ ìë™ ê´€ë¦¬ ì„ê³„ê°’ ì¡°ì •
**í˜„ì¬** (`backtester/optimiz.py:737`)  
```python
if std < hstd / 4:  # 75% ë‚®ì€ ê°’ ì œê±°
    del_list.append(var)
```

**ê°œì„ ì•ˆ: ë‹¨ê³„ë³„ ê°•ë„ ì¡°ì ˆ**  
```python
if k == 0:  # ì²« ë‹¨ê³„: ë³´ìˆ˜ì 
    threshold = hstd / 8  # 87.5% ì´í•˜ ì œê±°
elif k < 3:  # ì´ˆë°˜: ì ë‹¹íˆ
    threshold = hstd / 6  # 83.3% ì´í•˜ ì œê±°
else:  # í›„ë°˜: ê³µê²©ì 
    threshold = hstd / 3  # 66.7% ì´í•˜ ì œê±°
    
if std < threshold:
    del_list.append(var)
```

**íš¨ê³¼**: ì´ˆë°˜ì— ì¢‹ì€ ë²”ìœ„ë¥¼ ë„ˆë¬´ ë¹¨ë¦¬ ì œê±°í•˜ëŠ” ê²ƒ ë°©ì§€

---

### â­ 3. ë‹¤ë‹¨ê³„ ë²”ìœ„ í™•ì¥ ì „ëµ
**í˜„ì¬** (`backtester/optimiz.py:766-779`)  
```python
# ìµœì ê°’ì´ ê²½ê³„ì— ìˆì„ ë•Œë§Œ gapë§Œí¼ í™•ì¥
if high == first:
    new = first - gap
```

**ê°œì„ ì•ˆ**  
```python
# ìµœì ê°’ì´ ê²½ê³„ ê·¼ì²˜ 2ì¹¸ ì´ë‚´ì— ìˆìœ¼ë©´ í™•ì¥
if vars_[i][0].index(high) <= 1:  # ì•ìª½ ëì—ì„œ 2ì¹¸ ì´ë‚´
    new = first - gap * 2  # 2ë°° í™•ì¥
    if new not in total_del_list[i]:
        vars_[i][0] = [new, first - gap] + vars_[i][0]
        
elif vars_[i][0].index(high) >= len(vars_[i][0]) - 2:  # ë’¤ìª½ ëì—ì„œ 2ì¹¸ ì´ë‚´
    new = last + gap * 2
    if new not in total_del_list[i]:
        vars_[i][0] = vars_[i][0] + [last + gap, new]
```

**íš¨ê³¼**: ì „ì—­ ìµœì í•´ íƒìƒ‰ ë²”ìœ„ í™•ëŒ€

---

### â­ 4. Optuna Sampler ì¡°í•© ì „ëµ
**í˜„ì¬**: í•œ ê°€ì§€ ìƒ˜í”ŒëŸ¬ë§Œ ì‚¬ìš©

**ê°œì„ ì•ˆ: 2ë‹¨ê³„ í•˜ì´ë¸Œë¦¬ë“œ**
```python
def OptimizeOptunaHybrid(self, ...):
    # 1ë‹¨ê³„: TPESamplerë¡œ ë¹ ë¥¸ íƒìƒ‰ (ë³€ìˆ˜ê°œìˆ˜ Ã— 3)
    sampler1 = optuna.samplers.TPESampler()
    study1 = optuna.create_study(direction='maximize', sampler=sampler1)
    study1.optimize(objective, n_trials=len(self.vars) * 3)
    
    # 2ë‹¨ê³„: CmaEsSamplerë¡œ ì •ë°€ íƒìƒ‰ (ë³€ìˆ˜ê°œìˆ˜ Ã— 5)
    # 1ë‹¨ê³„ ìµœê³ ê°’ ì£¼ë³€ ë²”ìœ„ë¡œ ì¶•ì†Œ
    best_params = study1.best_params
    # ... ë²”ìœ„ ì¶•ì†Œ ë¡œì§ ...
    
    sampler2 = optuna.samplers.CmaEsSampler()
    study2 = optuna.create_study(direction='maximize', sampler=sampler2)
    study2.optimize(objective, n_trials=len(self.vars) * 5)
```

**íš¨ê³¼**: ì „ì—­ íƒìƒ‰ + ì§€ì—­ ì •ë°€ íƒìƒ‰ ì¡°í•©

---

### â­ 5. ë³€ìˆ˜ ì¤‘ìš”ë„ ê¸°ë°˜ íƒìƒ‰
**ì‹ ê·œ ê¸°ëŠ¥: ë³€ìˆ˜ ì˜í–¥ë„ ë¶„ì„**
```python
def AnalyzeVarImportance(self, turn_var_std):
    """ê° ë³€ìˆ˜ê°€ ê¸°ì¤€ê°’ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ë„ ê³„ì‚°"""
    importance = {}
    for vturn, var_std in turn_var_std.items():
        if len(var_std) > 1:
            std_values = list(var_std.values())
            # í‘œì¤€í¸ì°¨ê°€ í¬ë©´ ì¤‘ìš”í•œ ë³€ìˆ˜
            importance[vturn] = np.std(std_values) / np.mean(np.abs(std_values))
    
    # ìƒìœ„ 30% ë³€ìˆ˜ëŠ” ë²”ìœ„ í™•ëŒ€, í•˜ìœ„ 30%ëŠ” ë¹ ë¥¸ ê³ ì •
    sorted_vars = sorted(importance.items(), key=lambda x: x[1], reverse=True)
    high_impact = [v[0] for v in sorted_vars[:len(sorted_vars)//3]]
    low_impact = [v[0] for v in sorted_vars[-len(sorted_vars)//3:]]
    
    return high_impact, low_impact
```

**ê°œì„ ëœ ê·¸ë¦¬ë“œ ìµœì í™”**
```python
high_impact, low_impact = self.AnalyzeVarImportance(turn_var_std)

for vturn in high_impact:
    # ì¤‘ìš” ë³€ìˆ˜: ë²”ìœ„ 2ë°° í™•ì¥
    # ...
    
for vturn in low_impact:
    # ëœ ì¤‘ìš” ë³€ìˆ˜: ë¹ ë¥¸ ê³ ì •
    if k >= 2:  # 2ë‹¨ê³„ ì´í›„ ê³ ì •
        vars_[vturn] = [[high_var], high_var]
```

**íš¨ê³¼**: í•µì‹¬ ë³€ìˆ˜ì— ìµœì í™” ë¦¬ì†ŒìŠ¤ ì§‘ì¤‘

---

### â­ 6. Early Stopping ê°œì„ 
**í˜„ì¬** (`backtester/optimiz.py:363`)  
`last_num = best_num + len_vars  # ê³ ì • ì¶”ê°€ íšŸìˆ˜`

**ê°œì„ ì•ˆ: ì ì‘í˜• Early Stopping**
```python
def __call__(self, study, trial):
    best_opt = study.best_value
    best_num = study.best_trial.number
    curr_num = trial.number
    
    # ê°œì„ ìœ¨ ê¸°ë°˜ ë™ì  ì¡°ì •
    if curr_num > 10:
        recent_trials = [t.value for t in study.trials[-10:]]
        improvement_rate = (max(recent_trials) - min(recent_trials)) / max(recent_trials)
        
        if improvement_rate > 0.1:  # 10% ì´ìƒ ê°œì„  ì¤‘
            patience = self.len_vars * 2  # 2ë°° ë” íƒìƒ‰
        elif improvement_rate > 0.05:  # 5% ì´ìƒ
            patience = self.len_vars * 1.5
        else:  # 5% ë¯¸ë§Œ
            patience = self.len_vars * 0.5  # ë¹ ë¥¸ ì¢…ë£Œ
    else:
        patience = self.len_vars
    
    last_num = best_num + int(patience)
    # ...
```

**íš¨ê³¼**: ê°œì„  ê°€ëŠ¥ì„± ë†’ì„ ë•Œ ë” íƒìƒ‰, ë‚®ì„ ë•Œ ë¹ ë¥¸ ì¢…ë£Œ

---

### â­ 7. êµì°¨ê²€ì¦ ê°€ì¤‘ì¹˜ ìµœì í™”
**í˜„ì¬**: ê³ ì • ê°€ì¤‘ì¹˜ (2.0 â†’ 0.33)

**ê°œì„ ì•ˆ: ì ì‘í˜• ê°€ì¤‘ì¹˜**
```python
def GetAdaptiveWeights(train_data, valid_data, count):
    """TRAIN-VALID ìƒê´€ê´€ê³„ ê¸°ë°˜ ê°€ì¤‘ì¹˜ ì¡°ì •"""
    correlations = []
    for i in range(count):
        # ê° ë¶„í• ì˜ ì„±ëŠ¥ ì¼ê´€ì„± ì¸¡ì •
        if train_data[i] > 0 and valid_data[i] > 0:
            ratio = min(train_data[i], valid_data[i]) / max(train_data[i], valid_data[i])
            correlations.append(ratio)
        else:
            correlations.append(0)
    
    # ì¼ê´€ì„± ë†’ì€ ë¶„í• ì— ë†’ì€ ê°€ì¤‘ì¹˜
    weights = []
    for i in range(count):
        base_weight = (count - i) / count  # ìµœê·¼ ë°ì´í„° ì„ í˜¸
        consistency_weight = correlations[i]
        final_weight = base_weight * 0.6 + consistency_weight * 0.4
        weights.append(final_weight * 2)  # ìŠ¤ì¼€ì¼ ì¡°ì •
    
    return weights
```

**íš¨ê³¼**: ì¼ê´€ì„± ë†’ì€ ë¶„í• ì— ë” ë†’ì€ ê°€ì¤‘ì¹˜ ë¶€ì—¬

---

### â­ 8. ë©€í‹° ìŠ¤í… ê·¸ë¦¬ë“œ ìµœì í™”
**í˜„ì¬**: ë‹¨ì¼ ê°„ê²©(gap)ìœ¼ë¡œ ì „ì²´ ë²”ìœ„ íƒìƒ‰

**ê°œì„ ì•ˆ: ê±°ì¹œ íƒìƒ‰ â†’ ì •ë°€ íƒìƒ‰**
```python
def OptimizeGridMultiStep(self, ...):
    # 1ë‹¨ê³„: gap Ã— 3ìœ¼ë¡œ ë¹ ë¥¸ ì „ì—­ íƒìƒ‰
    coarse_vars = []
    for i, var in enumerate(vars_):
        if var[0][2] != 0:
            coarse_gap = var[0][2] * 3
            coarse_range = np.arange(var[0][0], var[0][1] + coarse_gap, coarse_gap)
            coarse_vars.append([list(coarse_range), var[1]])
        else:
            coarse_vars.append(var)
    
    # ê±°ì¹œ íƒìƒ‰ ì‹¤í–‰
    coarse_result = self.OptimizeGrid(..., coarse_vars, ...)
    
    # 2ë‹¨ê³„: ìµœì ê°’ ì£¼ë³€ gapìœ¼ë¡œ ì •ë°€ íƒìƒ‰
    fine_vars = []
    for i, var in enumerate(coarse_result):
        optimal = var[1]
        original_gap = vars_[i][0][2]
        fine_range = np.arange(optimal - original_gap * 3, 
                                optimal + original_gap * 3, 
                                original_gap)
        fine_vars.append([list(fine_range), optimal])
    
    # ì •ë°€ íƒìƒ‰ ì‹¤í–‰
    final_result = self.OptimizeGrid(..., fine_vars, ...)
    return final_result
```

**íš¨ê³¼**: íƒìƒ‰ ì‹œê°„ 50% ë‹¨ì¶•, ì „ì—­ ìµœì í•´ íƒì§€ìœ¨ í–¥ìƒ

---

### â­ 9. ì•™ìƒë¸” ìµœì í™” ì „ëµ
**ì‹ ê·œ**: ì—¬ëŸ¬ ìµœì í™” ê²°ê³¼ ì•™ìƒë¸”
```python
def OptimizeEnsemble(self, ...):
    results = []
    
    # 1) ê·¸ë¦¬ë“œ ìµœì í™”
    grid_vars = self.OptimizeGrid(...)
    grid_std = self.EvaluateVars(grid_vars)
    results.append((grid_vars, grid_std, 'Grid'))
    
    # 2) Optuna TPE
    tpe_vars = self.OptimizeOptuna(..., sampler='TPESampler')
    tpe_std = self.EvaluateVars(tpe_vars)
    results.append((tpe_vars, tpe_std, 'TPE'))
    
    # 3) Optuna CmaEs
    cma_vars = self.OptimizeOptuna(..., sampler='CmaEsSampler')
    cma_std = self.EvaluateVars(cma_vars)
    results.append((cma_vars, cma_std, 'CmaEs'))
    
    # 4) ìƒìœ„ 2ê°œ ì¡°í•©
    results.sort(key=lambda x: x[1], reverse=True)
    best1, best2 = results[0][0], results[1][0]
    
    # ë³€ìˆ˜ë³„ ê°€ì¤‘ í‰ê· 
    ensemble_vars = []
    for i in range(len(best1)):
        avg_val = (best1[i][1] * 0.6 + best2[i][1] * 0.4)
        ensemble_vars.append([[best1[i][0][0], best1[i][0][1], best1[i][0][2]], 
                              round(avg_val)])
    
    return ensemble_vars
```

**íš¨ê³¼**: ë‹¤ì–‘í•œ ìµœì í™” ë°©ë²•ì˜ ì¥ì  í†µí•©

---

### â­ 10. ì œí•œ ì¡°ê±´ ìš°ì„ ìˆœìœ„ ì ìš©
**í˜„ì¬**: 7ê°€ì§€ ì œí•œ ì¡°ê±´ AND ì—°ì‚°

**ê°œì„ ì•ˆ: ê°€ì¤‘ì¹˜ ê¸°ë°˜ ì ìˆ˜**
```python
def GetOptiStdTextWeighted(optistd, std_list, betting, result, pre_text):
    mdd_low, mdd_high, mhct_low, mhct_high, wr_low, wr_high, ap_low, ap_high, \
    atc_low, atc_high, cagr_low, cagr_high, tpi_low, tpi_high = std_list
    tc, atc, pc, mc, wr, ah, app, tpp, tsg, mhct, seed, cagr, tpi, mdd, mdd_ = result
    
    # ê° ì¡°ê±´ë³„ ë§Œì¡±ë„ ì ìˆ˜ (0~1)
    scores = []
    weights = []
    
    # ì¼í‰ê· ê±°ë˜íšŸìˆ˜ (ê°€ì¤‘ì¹˜: 0.25)
    if atc_low <= atc <= atc_high:
        atc_score = 1.0
    else:
        # ë²”ìœ„ ë²—ì–´ë‚œ ì •ë„ì— ë”°ë¼ í˜ë„í‹°
        if atc < atc_low:
            atc_score = max(0, atc / atc_low)
        else:
            atc_score = max(0, atc_high / atc)
    scores.append(atc_score)
    weights.append(0.25)
    
    # ìŠ¹ë¥  (ê°€ì¤‘ì¹˜: 0.20)
    if wr_low <= wr <= wr_high:
        wr_score = 1.0
    else:
        if wr < wr_low:
            wr_score = max(0, wr / wr_low)
        else:
            wr_score = max(0, wr_high / wr)
    scores.append(wr_score)
    weights.append(0.20)
    
    # ... ë‚˜ë¨¸ì§€ ì¡°ê±´ë“¤ ...
    
    # ì¢…í•© ì ìˆ˜
    total_score = sum([s * w for s, w in zip(scores, weights)])
    
    # ìµœì†Œ ì„ê³„ê°’ (0.7) ë¯¸ë§Œì´ë©´ í˜ë„í‹°
    if total_score < 0.7:
        std = tpp * total_score if std_true_partial else std_false_point
    else:
        std = tpp
    
    return std, text
```

**íš¨ê³¼**: ì¼ë¶€ ì¡°ê±´ ë¶ˆë§Œì¡± ì‹œì—ë„ ë¶€ë¶„ ì ìˆ˜ ë¶€ì—¬ (ê³¼ë„í•œ í•„í„°ë§ ë°©ì§€)

---

### â­ 11. ë³€ìˆ˜ íƒìƒ‰ ìˆœì„œ ìµœì í™”
**í˜„ì¬**: ìˆœì°¨ íƒìƒ‰ (`vars[0] â†’ vars[1] â†’ ...`)

**ê°œì„ ì•ˆ: ì¤‘ìš”ë„ ìˆœì„œ**
```python
def GetOptimizedSearchOrder(self, vars_):
    """ì´ì „ ìµœì í™” ì´ë ¥ ê¸°ë°˜ íƒìƒ‰ ìˆœì„œ ê²°ì •"""
    con = sqlite3.connect(DB_BACKTEST)
    df = pd.read_sql(f'SELECT * FROM ìµœì í™”ì´ë ¥ WHERE ì „ëµëª… = "{self.buystg_name}"', con)
    con.close()
    
    if len(df) > 0:
        # ì´ì „ ìµœì í™”ì—ì„œ ë³€ê²½ì´ ë§ì•˜ë˜ ë³€ìˆ˜ ìš°ì„ 
        var_changes = []
        for i in range(len(vars_)):
            changes = len(df[df[f'ë³€ìˆ˜{i}_ë³€ê²½'] == 1])
            var_changes.append((i, changes))
        
        # ë³€ê²½ ë§ì€ ìˆœì„œë¡œ ì •ë ¬
        var_changes.sort(key=lambda x: x[1], reverse=True)
        search_order = [x[0] for x in var_changes]
    else:
        # ê¸°ë³¸ ìˆœì„œ
        search_order = list(range(len(vars_)))
    
    return search_order
```

**íš¨ê³¼**: ì¤‘ìš”í•œ ë³€ìˆ˜ë¶€í„° íƒìƒ‰í•˜ì—¬ ë¹ ë¥¸ ìˆ˜ë ´

---

### â­ 12. ë¡œì»¬ ìµœì í•´ íƒˆì¶œ ë©”ì»¤ë‹ˆì¦˜
**ì‹ ê·œ**: ì •ì²´ ì‹œ ë¬´ì‘ìœ„ ì í”„
```python
def OptimizeGridWithJump(self, ...):
    stuck_count = 0
    prev_hstd = 0
    
    for k in range(ccount):
        # ... ê¸°ì¡´ ìµœì í™” ë¡œì§ ...
        
        # ê°œì„  ì •ì²´ ê°ì§€
        if abs(hstd - prev_hstd) / max(abs(prev_hstd), 1) < 0.01:  # 1% ë¯¸ë§Œ ê°œì„ 
            stuck_count += 1
        else:
            stuck_count = 0
        
        prev_hstd = hstd
        
        # 3íšŒ ì—°ì† ì •ì²´ ì‹œ ë¬´ì‘ìœ„ ì í”„
        if stuck_count >= 3:
            self.wq.put((ui_num[f'{self.ui_gubun}ë°±í…ŒìŠ¤íŠ¸'], 
                        f'ë¡œì»¬ ìµœì í•´ íƒˆì¶œ ì‹œë„ - ë¬´ì‘ìœ„ ì í”„ [{k+1}]ë‹¨ê³„'))
            
            # 30% ë³€ìˆ˜ë¥¼ ë¬´ì‘ìœ„ ê°’ìœ¼ë¡œ ë³€ê²½
            num_jump = max(1, len(vars_) // 3)
            jump_indices = random.sample(range(len(vars_)), num_jump)
            
            for idx in jump_indices:
                if len(vars_[idx][0]) > 1:
                    vars_[idx][1] = random.choice(vars_[idx][0])
            
            stuck_count = 0
```

**íš¨ê³¼**: ë¡œì»¬ ìµœì í•´ì— ê°‡íˆëŠ” ê²ƒ ë°©ì§€

---

### â­ 13. ë°°ì¹˜ í¬ê¸° ë™ì  ì¡°ì •
**í˜„ì¬**: ê³ ì •ëœ 20ê°œ í”„ë¡œì„¸ìŠ¤

**ê°œì„ ì•ˆ: CPU/ë©”ëª¨ë¦¬ ìƒí™© ê¸°ë°˜ ì¡°ì •**
```python
def GetOptimalBatchSize(self):
    """ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ê¸°ë°˜ ìµœì  ë°°ì¹˜ í¬ê¸° ê²°ì •"""
    import psutil
    
    cpu_count = psutil.cpu_count()
    mem_available = psutil.virtual_memory().available / (1024**3)  # GB
    
    # CPU ê¸°ë°˜
    max_by_cpu = int(cpu_count * 0.8)  # CPU 80% í™œìš©
    
    # ë©”ëª¨ë¦¬ ê¸°ë°˜ (í”„ë¡œì„¸ìŠ¤ë‹¹ 500MB ê°€ì •)
    max_by_mem = int(mem_available / 0.5)
    
    # ë‘˜ ì¤‘ ì‘ì€ ê°’
    optimal_batch = min(max_by_cpu, max_by_mem, 30)  # ìµœëŒ€ 30
    optimal_batch = max(optimal_batch, 5)  # ìµœì†Œ 5
    
    return optimal_batch
```

**íš¨ê³¼**: ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ìµœì  í™œìš©

---

### â­ 14. ì¤‘ë³µ ê³„ì‚° ìºì‹± ê°•í™”
**í˜„ì¬**: Optunaì—ì„œë§Œ ì¤‘ë³µ ë°©ì§€

**ê°œì„ ì•ˆ: ì „ì—­ ìºì‹œ**
```python
class GlobalVarsCache:
    def __init__(self):
        self.cache = {}  # {str(vars): std}
        self.cache_file = './_database/optim_cache.pkl'
        self.LoadCache()
    
    def LoadCache(self):
        if os.path.exists(self.cache_file):
            with open(self.cache_file, 'rb') as f:
                self.cache = pickle.load(f)
    
    def Get(self, vars_list):
        key = str(sorted(vars_list))
        return self.cache.get(key, None)
    
    def Set(self, vars_list, std):
        key = str(sorted(vars_list))
        self.cache[key] = std
        
        # 1000ê°œë§ˆë‹¤ ë””ìŠ¤í¬ ì €ì¥
        if len(self.cache) % 1000 == 0:
            with open(self.cache_file, 'wb') as f:
                pickle.dump(self.cache, f)
    
    def GetHitRate(self):
        return self.hits / (self.hits + self.misses)

# ëª¨ë“  ìµœì í™” ë©”ì„œë“œì— ì ìš©
global_cache = GlobalVarsCache()

def OptimizeGrid(self, ...):
    for ...:
        cached_std = global_cache.Get(curr_vars)
        if cached_std is not None:
            # ìºì‹œ íˆíŠ¸
            std = cached_std
        else:
            # ë°±í…ŒìŠ¤íŒ… ì‹¤í–‰
            # ...
            global_cache.Set(curr_vars, std)
```

**íš¨ê³¼**: ì¤‘ë³µ ê³„ì‚° ì œê±°ë¡œ ì†ë„ 20-30% í–¥ìƒ

---

### â­ 15. ìµœì í™” ë©”íƒ€ ëŸ¬ë‹
**ì‹ ê·œ**: ê³¼ê±° ìµœì í™” ì´ë ¥ í•™ìŠµ
```python
def MetaLearning(self):
    """ê³¼ê±° ìµœì í™” ê²°ê³¼ ë¶„ì„í•˜ì—¬ íš¨ê³¼ì ì¸ ë²”ìœ„ ì˜ˆì¸¡"""
    con = sqlite3.connect(DB_BACKTEST)
    df = pd.read_sql('SELECT * FROM ìµœì í™”ì´ë ¥ ORDER BY ì‹¤í–‰ì‹œê°„ DESC LIMIT 100', con)
    con.close()
    
    if len(df) < 10:
        return None
    
    # 1) ìµœì ê°’ ë¶„í¬ ë¶„ì„
    optimal_distributions = {}
    for i in range(len(self.vars)):
        values = df[f'ë³€ìˆ˜{i}_ìµœì ê°’'].dropna()
        if len(values) > 5:
            mean = values.mean()
            std = values.std()
            optimal_distributions[i] = {
                'mean': mean,
                'std': std,
                'min': values.min(),
                'max': values.max()
            }
    
    # 2) ë²”ìœ„ ì¶”ì²œ
    recommended_ranges = {}
    for i, dist in optimal_distributions.items():
        # í‰ê·  Â± 2Ïƒ ë²”ìœ„
        low = max(self.vars[i][0][0], dist['mean'] - 2 * dist['std'])
        high = min(self.vars[i][0][1], dist['mean'] + 2 * dist['std'])
        recommended_ranges[i] = [low, high]
    
    # 3) ì‚¬ìš©ìì—ê²Œ ì¶”ì²œ
    self.wq.put((ui_num[f'{self.ui_gubun}ë°±í…ŒìŠ¤íŠ¸'], 
                f'ë©”íƒ€ ëŸ¬ë‹ ì¶”ì²œ ë²”ìœ„: {recommended_ranges}'))
    
    return recommended_ranges
```

**íš¨ê³¼**: ê³¼ê±° íŒ¨í„´ í•™ìŠµìœ¼ë¡œ ì´ˆê¸° ë²”ìœ„ ì„¤ì • ê°œì„ 

---

## ğŸ“ˆ ìš°ì„ ìˆœìœ„ë³„ ì ìš© ìˆœì„œ
- ğŸ”¥ ì¦‰ì‹œ ì ìš© (High Impact, Low Effort)  
  êµì°¨ê²€ì¦ MERGE ê³„ì‚° ë°©ì‹ ê°œì„ (ì¡°í™”í‰ê· ) / ë²”ìœ„ ìë™ ê´€ë¦¬ ì„ê³„ê°’ ì¡°ì •(ë‹¨ê³„ë³„ ê°•ë„) / ì¤‘ë³µ ê³„ì‚° ìºì‹± ê°•í™”
- âš¡ ë‹¨ê¸° ì ìš© (1-2ì£¼)  
  ë‹¤ë‹¨ê³„ ë²”ìœ„ í™•ì¥ ì „ëµ / Optuna Sampler ì¡°í•© ì „ëµ / Early Stopping ê°œì„  / ë³€ìˆ˜ ì¤‘ìš”ë„ ê¸°ë°˜ íƒìƒ‰
- ğŸ¯ ì¤‘ê¸° ì ìš© (1-2ê°œì›”)  
  ë©€í‹° ìŠ¤í… ê·¸ë¦¬ë“œ ìµœì í™” / ë¡œì»¬ ìµœì í•´ íƒˆì¶œ ë©”ì»¤ë‹ˆì¦˜ / êµì°¨ê²€ì¦ ê°€ì¤‘ì¹˜ ìµœì í™” / ì œí•œ ì¡°ê±´ ìš°ì„ ìˆœìœ„ ì ìš©
- ğŸš€ ì¥ê¸° ì ìš© (3ê°œì›”+)  
  ì•™ìƒë¸” ìµœì í™” ì „ëµ / ë³€ìˆ˜ íƒìƒ‰ ìˆœì„œ ìµœì í™” / ë°°ì¹˜ í¬ê¸° ë™ì  ì¡°ì • / ìµœì í™” ë©”íƒ€ ëŸ¬ë‹

---

## ğŸ’¡ ì˜ˆìƒ íš¨ê³¼
- ì„±ëŠ¥ ê°œì„ : ìˆ˜ìµë¥  15-30% í–¥ìƒ(ë” ê· í˜•ì¡íŒ ì „ëµ), ìµœëŒ€ë‚™í­ë¥  20-40% ê°ì†Œ, TRAIN-VALID ìƒê´€ë„ 0.8+ ë‹¬ì„±
- íš¨ìœ¨ì„± ê°œì„ : ìµœì í™” ì‹œê°„ 30-50% ë‹¨ì¶•, ìºì‹œ íˆíŠ¸ìœ¨ 20-30%, ë¦¬ì†ŒìŠ¤ í™œìš©(CPU) 80%+ ìœ ì§€
